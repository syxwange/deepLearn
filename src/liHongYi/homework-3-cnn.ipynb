{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CNN对图片进行分类"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import torchvision.transforms as transforms\r\n",
    "from torchvision.datasets import ImageFolder ,DatasetFolder\r\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset,Dataset\r\n",
    "import torch\r\n",
    "from tqdm.auto import tqdm\r\n",
    "import torchvision"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset, Data Loader, and Transforms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainTfm =transforms.Compose([transforms.Resize((128,128)),\r\n",
    "\r\n",
    "transforms.ToTensor(),\r\n",
    "#transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n",
    "])\r\n",
    "testTfm =transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#? 更大的批量通常会产生更稳定的梯度\r\n",
    "batch_size = 128\r\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "trainSet = ImageFolder(\"../data/food-11/training/labeled/\",transform=trainTfm)\r\n",
    "unlabelSet = ImageFolder(\"../data/food-11/training/unlabeled/\",transform=trainTfm)\r\n",
    "validSet = ImageFolder(\"../data/food-11/validation/\",transform=testTfm)\r\n",
    "testSet = ImageFolder(\"../data/food-11/testing/\",transform=testTfm)\r\n",
    "\r\n",
    "pm = True if device==\"cuda:0\" else False   #如果是GPU 把图片加入到CUDA中的固定内存\r\n",
    "trainLoader = DataLoader(trainSet,batch_size=batch_size,shuffle=True,num_workers=8, pin_memory=pm)\r\n",
    "validLoader = DataLoader(validSet,batch_size=batch_size,shuffle=True,num_workers=8, pin_memory=pm)\r\n",
    "testLoader = DataLoader(testSet,batch_size=batch_size,num_workers=8, pin_memory=pm)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modle"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CNNModel(torch.nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.cnnLayers = torch.nn.Sequential(\r\n",
    "            torch.nn.Conv2d(3, 64, 3, 1, 1),\r\n",
    "            torch.nn.BatchNorm2d(64),\r\n",
    "            torch.nn.ReLU(),\r\n",
    "            torch.nn.MaxPool2d(2, 2, 0),\r\n",
    "\r\n",
    "            torch.nn.Conv2d(64, 128, 3, 1, 1),\r\n",
    "            torch.nn.BatchNorm2d(128),\r\n",
    "            torch.nn.ReLU(),\r\n",
    "            torch.nn.MaxPool2d(2, 2, 0),\r\n",
    "\r\n",
    "            torch.nn.Conv2d(128, 256, 3, 1, 1),\r\n",
    "            torch.nn.BatchNorm2d(256),\r\n",
    "            torch.nn.ReLU(),\r\n",
    "            torch.nn.MaxPool2d(4, 4, 0),\r\n",
    "        )\r\n",
    "        self.fcLayers = torch.nn.Sequential(\r\n",
    "            torch.nn.Linear(256 * 8 * 8, 256),\r\n",
    "            torch.nn.ReLU(),\r\n",
    "            torch.nn.Linear(256, 256),\r\n",
    "            torch.nn.ReLU(),\r\n",
    "            torch.nn.Linear(256, 11)\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, x):      \r\n",
    "        x = self.cnnLayers(x)\r\n",
    "        x = x.flatten(1)\r\n",
    "        x = self.fcLayers(x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class customSubset(Dataset):\r\n",
    "    #自定义DataSet,用于半监督学习未标签的图片，在训练后生成伪标签后组成数据集\r\n",
    "    def __init__(self, dataset, indices, labels):\r\n",
    "        self.dataset = torch.utils.data.Subset(dataset, indices)\r\n",
    "        self.targets = labels\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        image = self.dataset[idx][0]\r\n",
    "        target = self.targets[idx]\r\n",
    "        return (image, target)\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.targets)\r\n",
    "\r\n",
    "\r\n",
    "def getPseudoLabels(dataset, model, threshold=0.65):\r\n",
    "    # 此函数使用给定的模型生成数据集的伪标签\r\n",
    "    # 它返回一个DatasetFolder实例，其中包含预测可信度超过给定阈值的图像 \r\n",
    "   \r\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False) \r\n",
    "    model.eval()    \r\n",
    "    softmax = torch.nn.Softmax(dim=-1)\r\n",
    "    \r\n",
    "    for batch in tqdm(data_loader):\r\n",
    "        img, _ = batch      \r\n",
    "        with torch.no_grad():\r\n",
    "            logits = model(img.to(device))\r\n",
    "        # Obtain the probability distributions by applying softmax on logits.\r\n",
    "        probs = softmax(logits)\r\n",
    "        # ---------- TODO ----------\r\n",
    "        # Filter the data and construct a new dataset.\r\n",
    "        \r\n",
    "\r\n",
    "    # # Turn off the eval mode.\r\n",
    "    model.train()\r\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = CNNModel().to(device)\r\n",
    "opt = torch.optim.Adam(model.parameters(),lr=0.0003,weight_decay=1e-5)\r\n",
    "criterion = torch.nn.CrossEntropyLoss() \r\n",
    "epochs = 80\r\n",
    "doSemi = False\r\n",
    "bestAcc = 0\r\n",
    "trainLossList ,trainAccList,validLossList,validAccList = [],[],[],[]\r\n",
    "for epoch in  tqdm(range(epochs)):    \r\n",
    "    if doSemi:\r\n",
    "        pseudoSet = getPseudoLabels(unlabelSet,model)\r\n",
    "        concatSet = ConcatDataset([trainSet,pseudoSet])\r\n",
    "        trainLoader = DataLoader(concatSet,batch_size=batch_size,shuffle=True,num_workers=8,pin_memory=pm)\r\n",
    "\r\n",
    "    model.train()\r\n",
    "    trainLoss, trainAcc= [],[]\r\n",
    "    for imgs,labels in trainLoader:\r\n",
    "        ret = model(imgs.to(device))\r\n",
    "        loss = criterion(ret,labels.to(device))\r\n",
    "        opt.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        #?梯度减切Gradient Clip设置一个梯度减切的阈值，如果在更新梯度的时候，\r\n",
    "        #? 梯度超过这个阈值，则会将其限制在这个范围之内，防止梯度爆炸。\r\n",
    "        gradNorm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\r\n",
    "        opt.step()\r\n",
    "        acc = (torch.argmax(ret,dim=1)==labels.to(device)).float().mean()\r\n",
    "        trainLoss.append(loss)\r\n",
    "        trainAcc.append(acc)\r\n",
    "\r\n",
    "    trainLoss,trainAcc = sum(trainLoss)/len(trainLoss),sum(trainAcc)/len(trainAcc)  \r\n",
    "    trainLossList.append(trainLoss)\r\n",
    "    trainAccList.append(trainAcc)\r\n",
    "\r\n",
    "    model.eval()\r\n",
    "    validLoss ,validAcc= [],[]\r\n",
    "    for imgs,labels in validLoader:\r\n",
    "        with torch.no_grad():\r\n",
    "            ret = model(imgs.to(device))\r\n",
    "            loss = criterion(ret,labels.to(device))\r\n",
    "        acc = (torch.argmax(ret,dim=1)==labels.to(device)).float().mean()\r\n",
    "        validLoss.append(loss)\r\n",
    "        validAcc.append(acc)\r\n",
    "\r\n",
    "    validLoss ,validAcc= sum(validLoss)/len(validLoss),sum(validAcc)/len(validAcc)     \r\n",
    "    validLossList.append(validLoss)\r\n",
    "    validAccList.append(validAcc)\r\n",
    "\r\n",
    "    if epoch%5==0:\r\n",
    "        print(f\"Train--epoch:{epoch}/{epochs}--loss={trainLoss:.5f}--acc={trainAcc:.5f}\")\r\n",
    "        print(f\"*********Valid--loss={validLoss:.5f}--acc={validAcc:.5f}\")\r\n",
    "\r\n",
    "    if validAcc > bestAcc:\r\n",
    "        bestAcc = validAcc\r\n",
    "        torch.save(model.state_dict(),\"./bestAcc.ckpt\")\r\n",
    "        \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "vgg = torchvision.models.vgg11_bn()\r\n",
    "vgg.features"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU(inplace=True)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "aa = vgg.classifier\r\n",
    "aa._modules['0'] =torch.nn.Linear(in_features=512*4*4, out_features=4096, bias=True)\r\n",
    "aa._modules['6'] =torch.nn.Linear(in_features=4096, out_features=11, bias=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "aa"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "10225e230a0ef510396827da287f7cf92eba04588ef8397eb0b6ba209c02b811"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}