{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 任务描述\r\n",
    "- 对特定特征的说话人进行分类。\r\n",
    "- 主要目标：学习如何使用transformer。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import os\r\n",
    "import json\r\n",
    "import torch\r\n",
    "import random\r\n",
    "from pathlib import Path\r\n",
    "from torch.utils.data import Dataset ,DataLoader, random_split\r\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset DataLorder\r\n",
    "- Original dataset is [Voxceleb1](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class myDataset(Dataset):\r\n",
    "  def __init__(self, data_dir, segment_len=128):\r\n",
    "    self.data_dir  ,self.segment_len=data_dir ,segment_len \r\n",
    "    \r\n",
    "    speaker2id = json.load(open(data_dir+\"/mapping.json\"))[\"speaker2id\"]    \r\n",
    "    metadata = json.load(open(data_dir+\"/metadata.json\"))[\"speakers\"] \r\n",
    "    self.speaker_num = len(metadata.keys())\r\n",
    "    self.data = []\r\n",
    "    for speaker in metadata.keys():\r\n",
    "      for utterances in metadata[speaker]:\r\n",
    "        self.data.append([utterances[\"feature_path\"], speaker2id[speaker]])\r\n",
    " \r\n",
    "  def __len__(self):\r\n",
    "    return len(self.data)\r\n",
    " \r\n",
    "  def __getitem__(self, index):\r\n",
    "    feat_path, speaker = self.data[index]\r\n",
    "    # Load preprocessed mel-spectrogram.\r\n",
    "    mel = torch.load(os.path.join(self.data_dir, feat_path))\r\n",
    "    #最大长度限制在segment_len\r\n",
    "    if len(mel) > self.segment_len:      \r\n",
    "      start = random.randint(0, len(mel) - self.segment_len)      \r\n",
    "      mel = torch.FloatTensor(mel[start:start+self.segment_len])\r\n",
    "    else:\r\n",
    "      mel = torch.FloatTensor(mel)    \r\n",
    "    speaker = torch.FloatTensor([speaker]).long()\r\n",
    "    return mel, speaker\r\n",
    " \r\n",
    "  def get_speaker_number(self):\r\n",
    "    return self.speaker_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Datalorder\r\n",
    "- 将数据集分为训练数据集（90%）和验证数据集（10%）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def collate_batch(batch):\r\n",
    "  # Process features within a batch.\r\n",
    "  \"\"\"Collate a batch of data.\"\"\"\r\n",
    "  print(len(batch))\r\n",
    "  mel, speaker = zip(*batch)\r\n",
    "  print(mel.shape)\r\n",
    "  # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\r\n",
    "  mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\r\n",
    "  # mel: (batch size, length, 40)\r\n",
    "  return mel, torch.FloatTensor(speaker).long()\r\n",
    "\r\n",
    "def get_dataloader(data_dir, batch_size, n_workers):\r\n",
    "  \"\"\"生成数据加载器\"\"\"\r\n",
    "  dataset = myDataset(data_dir)\r\n",
    "  speaker_num = dataset.get_speaker_number()\r\n",
    "  # 将数据集拆分为训练数据集和验证数据集\r\n",
    "  trainlen = int(0.9 * len(dataset))  \r\n",
    "  trainset, validset = random_split(dataset, [trainlen, len(dataset) - trainlen])\r\n",
    "\r\n",
    "  train_loader = DataLoader(trainset, batch_size=batch_size,  shuffle=True,  drop_last=True,\r\n",
    "    num_workers=n_workers,  pin_memory=True,   collate_fn=collate_batch,  )\r\n",
    "  valid_loader = DataLoader( validset,  batch_size=batch_size,  num_workers=n_workers,\r\n",
    "    drop_last=True,  pin_memory=True,  collate_fn=collate_batch, )\r\n",
    "\r\n",
    "  return train_loader, valid_loader, speaker_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "aa = myDataset('../../data/Dataset')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "bb = get_dataloader('../../data/Dataset',20,4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "for i in bb:\r\n",
    "    len(i)\r\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 25, 300])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "10225e230a0ef510396827da287f7cf92eba04588ef8397eb0b6ba209c02b811"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}